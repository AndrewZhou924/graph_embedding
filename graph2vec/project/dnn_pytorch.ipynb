{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data processing\n",
    "'''\n",
    "BATCH_SIZE = 256\n",
    "class FaceLandmarksDataset(data.Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "    def __init__(self, csv_file,datatype='train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file, iterator=True)\n",
    "        self.datatype = datatype\n",
    "        self.csv_file = csv_file\n",
    "        self.epoch = 1\n",
    "        self.trainNum = 26518\n",
    "        self.testNum = 5303\n",
    "        \n",
    "        #Todo \n",
    "#         self.landmarks_frame[] # set trainset (0.7)\n",
    "        \n",
    "    def __len__(self):\n",
    "        #print len(self.landmarks_frame)\n",
    "        #return len(self.landmarks_frame)\n",
    "        if self.datatype == 'train':\n",
    "            return int(self.trainNum/BATCH_SIZE)+1\n",
    "        if self.datatype == 'test':\n",
    "            return int(self.testNum/BATCH_SIZE)+1\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         print(idx)\n",
    "        landmarks = self.landmarks_frame.get_chunk(BATCH_SIZE).as_matrix().astype('float')\n",
    "        # landmarks = self.landmarks_frame.ix[idx, 1:].as_matrix().astype('float')\n",
    " \n",
    "        # 采用这个，不错。\n",
    "        return landmarks\n",
    "    def refresh(self):\n",
    "        print(\"refreshing...\")\n",
    "        self.epoch += 1\n",
    "        self.landmarks_frame = pd.read_csv(self.csv_file, iterator=True)\n",
    "        \n",
    "    def getTrainNum(self):\n",
    "        return int(self.trainNum/BATCH_SIZE)+1\n",
    "    \n",
    "    def getTestNum(self):\n",
    "        return int(self.testNum/BATCH_SIZE)+1\n",
    "\n",
    "filename = './trainset.csv'\n",
    "dataset = FaceLandmarksDataset(filename,'train')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "testname = './testset.csv'\n",
    "testset = FaceLandmarksDataset(testname,'test')\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_data in dataloader:\n",
    "#     print(shuffle(batch_data[0][:,256]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):     # 继承 torch 的 Module\n",
    "    def __init__(self, n_feature):\n",
    "        super(Net, self).__init__()     # 继承 __init__ 功能\n",
    "        self.hidden1 = torch.nn.Linear(n_feature, 128)   # 隐藏层线性输出\n",
    "        self.hidden2 = torch.nn.Linear(128, 64)\n",
    "        self.hidden3 = torch.nn.Linear(64, 32)\n",
    "        self.out = torch.nn.Linear(32, 2)       # 输出层线性输出\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 正向传播输入值, 神经网络分析出输出值\n",
    "        x = F.relu(self.hidden1(x))      # 激励函数(隐藏层的线性值)\n",
    "        x = F.relu(self.hidden2(x)) \n",
    "        x = F.relu(self.hidden3(x)) \n",
    "        x = self.out(x)                 # 输出值, 但是这个不是预测值, 预测值还需要再另外计算\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "net = Net(n_feature=128*2) # 几个类别就几个 output\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # the target label is NOT an one-hotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_data in dataloader:\n",
    "#     print(batch_data[0][:,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "\n",
    "def train(epoch):\n",
    "    print('\\ntraining Epoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i = 0\n",
    "    for batch_data in dataloader:\n",
    "        inputs = batch_data[0][:,1:257]\n",
    "        inputs = inputs.float()\n",
    "        targets = batch_data[0][:,257]\n",
    "        targets = targets.long()\n",
    "  \n",
    "        if inputs.shape[0] < BATCH_SIZE:\n",
    "            dataset.refresh()\n",
    "            \n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "#         outputs = outputs.long()\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # print(targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        \n",
    "        if i >= dataset.getTrainNum() - 1:\n",
    "            print('train set:Loss: %.5f | Acc: %.5f%% (%d/%d)'\n",
    "                % (train_loss/(i+1), 100.*correct/total, correct, total))\n",
    "        i += 1\n",
    "    \n",
    "def test(epoch):\n",
    "    print('testing Epoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i = 0\n",
    "    for batch_data in testloader:\n",
    "        inputs = batch_data[0][:,1:257]\n",
    "        inputs = inputs.float()\n",
    "        targets = batch_data[0][:,257]\n",
    "        targets = targets.long()\n",
    "  \n",
    "        if inputs.shape[0] < BATCH_SIZE:\n",
    "            testset.refresh()\n",
    "            \n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        \n",
    "        if i >= dataset.getTestNum() - 1:\n",
    "            print('test set:Loss: %.5f | Acc: %.5f%% (%d/%d)'\n",
    "                % (train_loss/(i+1), 100.*correct/total, correct, total))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training Epoch: 0\n",
      "refreshing...\n",
      "train set:Loss: 0.68021 | Acc: 56.67778% (14955/26386)\n",
      "testing Epoch: 0\n",
      "test set:Loss: 0.68738 | Acc: 55.80357% (3000/5376)\n",
      "\n",
      "training Epoch: 1\n",
      "refreshing...\n",
      "train set:Loss: 0.67747 | Acc: 57.42060% (15151/26386)\n",
      "testing Epoch: 1\n",
      "refreshing...\n",
      "test set:Loss: 0.67636 | Acc: 57.54126% (3033/5271)\n",
      "\n",
      "training Epoch: 2\n",
      "refreshing...\n",
      "train set:Loss: 0.67755 | Acc: 57.38270% (15141/26386)\n",
      "testing Epoch: 2\n",
      "refreshing...\n",
      "test set:Loss: 0.68612 | Acc: 56.53576% (2980/5271)\n",
      "\n",
      "training Epoch: 3\n",
      "refreshing...\n",
      "train set:Loss: 0.67815 | Acc: 57.20458% (15094/26386)\n",
      "testing Epoch: 3\n",
      "test set:Loss: 0.67861 | Acc: 56.45461% (3035/5376)\n",
      "\n",
      "training Epoch: 4\n",
      "refreshing...\n",
      "train set:Loss: 0.67531 | Acc: 57.63663% (15208/26386)\n",
      "testing Epoch: 4\n",
      "refreshing...\n",
      "test set:Loss: 0.67520 | Acc: 57.10491% (3010/5271)\n",
      "\n",
      "training Epoch: 5\n",
      "refreshing...\n",
      "train set:Loss: 0.67538 | Acc: 57.58357% (15194/26386)\n",
      "testing Epoch: 5\n",
      "refreshing...\n",
      "test set:Loss: 0.67476 | Acc: 58.37602% (3077/5271)\n",
      "\n",
      "training Epoch: 6\n",
      "refreshing...\n",
      "train set:Loss: 0.67233 | Acc: 58.11415% (15334/26386)\n",
      "testing Epoch: 6\n",
      "refreshing...\n",
      "test set:Loss: 0.67851 | Acc: 56.27016% (2966/5271)\n",
      "\n",
      "training Epoch: 7\n",
      "refreshing...\n",
      "train set:Loss: 0.67123 | Acc: 58.20132% (15357/26386)\n",
      "testing Epoch: 7\n",
      "test set:Loss: 0.68129 | Acc: 55.74777% (2997/5376)\n",
      "\n",
      "training Epoch: 8\n",
      "refreshing...\n",
      "train set:Loss: 0.67174 | Acc: 58.15584% (15345/26386)\n",
      "testing Epoch: 8\n",
      "refreshing...\n",
      "test set:Loss: 0.66866 | Acc: 59.15386% (3118/5271)\n",
      "\n",
      "training Epoch: 9\n",
      "refreshing...\n",
      "train set:Loss: 0.66888 | Acc: 58.45145% (15423/26386)\n",
      "testing Epoch: 9\n",
      "refreshing...\n",
      "test set:Loss: 0.67732 | Acc: 57.48435% (3030/5271)\n",
      "\n",
      "training Epoch: 10\n",
      "refreshing...\n",
      "train set:Loss: 0.66944 | Acc: 58.25438% (15371/26386)\n",
      "testing Epoch: 10\n",
      "test set:Loss: 0.67133 | Acc: 58.29613% (3134/5376)\n",
      "\n",
      "training Epoch: 11\n",
      "refreshing...\n",
      "train set:Loss: 0.66878 | Acc: 57.98151% (15299/26386)\n",
      "testing Epoch: 11\n",
      "refreshing...\n",
      "test set:Loss: 0.67469 | Acc: 58.73648% (3096/5271)\n",
      "\n",
      "training Epoch: 12\n",
      "refreshing...\n",
      "train set:Loss: 0.66927 | Acc: 58.09520% (15329/26386)\n",
      "testing Epoch: 12\n",
      "refreshing...\n",
      "test set:Loss: 0.68294 | Acc: 54.35401% (2865/5271)\n",
      "\n",
      "training Epoch: 13\n",
      "refreshing...\n",
      "train set:Loss: 0.67019 | Acc: 58.06109% (15320/26386)\n",
      "testing Epoch: 13\n",
      "refreshing...\n",
      "test set:Loss: 0.67101 | Acc: 58.75545% (3097/5271)\n",
      "\n",
      "training Epoch: 14\n",
      "refreshing...\n",
      "train set:Loss: 0.66646 | Acc: 58.67127% (15481/26386)\n",
      "testing Epoch: 14\n",
      "test set:Loss: 0.67122 | Acc: 57.94271% (3115/5376)\n",
      "\n",
      "training Epoch: 15\n",
      "refreshing...\n",
      "train set:Loss: 0.66386 | Acc: 58.49693% (15435/26386)\n",
      "testing Epoch: 15\n",
      "refreshing...\n",
      "test set:Loss: 0.67387 | Acc: 55.92867% (2948/5271)\n",
      "\n",
      "training Epoch: 16\n",
      "refreshing...\n",
      "train set:Loss: 0.66665 | Acc: 58.30365% (15384/26386)\n",
      "testing Epoch: 16\n",
      "refreshing...\n",
      "test set:Loss: 0.66755 | Acc: 57.10491% (3010/5271)\n",
      "\n",
      "training Epoch: 17\n",
      "refreshing...\n",
      "train set:Loss: 0.66426 | Acc: 58.52725% (15443/26386)\n",
      "testing Epoch: 17\n",
      "refreshing...\n",
      "test set:Loss: 0.67251 | Acc: 58.86928% (3103/5271)\n",
      "\n",
      "training Epoch: 18\n",
      "refreshing...\n",
      "train set:Loss: 0.65819 | Acc: 59.38376% (15669/26386)\n",
      "testing Epoch: 18\n",
      "test set:Loss: 0.66552 | Acc: 57.81250% (3108/5376)\n",
      "\n",
      "training Epoch: 19\n",
      "refreshing...\n",
      "train set:Loss: 0.66700 | Acc: 58.37186% (15402/26386)\n",
      "testing Epoch: 19\n",
      "refreshing...\n",
      "test set:Loss: 0.66651 | Acc: 58.01556% (3058/5271)\n",
      "\n",
      "training Epoch: 20\n",
      "refreshing...\n",
      "train set:Loss: 0.66076 | Acc: 58.77738% (15509/26386)\n",
      "testing Epoch: 20\n",
      "refreshing...\n",
      "test set:Loss: 0.66972 | Acc: 56.36502% (2971/5271)\n",
      "\n",
      "training Epoch: 21\n",
      "refreshing...\n",
      "train set:Loss: 0.65902 | Acc: 59.03509% (15577/26386)\n",
      "testing Epoch: 21\n",
      "test set:Loss: 0.66437 | Acc: 58.89137% (3166/5376)\n",
      "\n",
      "training Epoch: 22\n",
      "refreshing...\n",
      "train set:Loss: 0.65901 | Acc: 58.63716% (15472/26386)\n",
      "testing Epoch: 22\n",
      "refreshing...\n",
      "test set:Loss: 0.65935 | Acc: 59.30563% (3126/5271)\n",
      "\n",
      "training Epoch: 23\n",
      "refreshing...\n",
      "train set:Loss: 0.65598 | Acc: 59.40650% (15675/26386)\n",
      "testing Epoch: 23\n",
      "refreshing...\n",
      "test set:Loss: 0.66815 | Acc: 58.60368% (3089/5271)\n",
      "\n",
      "training Epoch: 24\n",
      "refreshing...\n",
      "train set:Loss: 0.66291 | Acc: 58.26575% (15374/26386)\n",
      "testing Epoch: 24\n",
      "refreshing...\n",
      "test set:Loss: 0.66494 | Acc: 58.71751% (3095/5271)\n",
      "\n",
      "training Epoch: 25\n",
      "refreshing...\n",
      "train set:Loss: 0.65812 | Acc: 59.08815% (15591/26386)\n",
      "testing Epoch: 25\n",
      "test set:Loss: 0.66442 | Acc: 58.87277% (3165/5376)\n",
      "\n",
      "training Epoch: 26\n",
      "refreshing...\n",
      "train set:Loss: 0.65421 | Acc: 59.26249% (15637/26386)\n",
      "testing Epoch: 26\n",
      "refreshing...\n",
      "test set:Loss: 0.66518 | Acc: 57.76892% (3045/5271)\n",
      "\n",
      "training Epoch: 27\n",
      "refreshing...\n",
      "train set:Loss: 0.65743 | Acc: 58.59168% (15460/26386)\n",
      "testing Epoch: 27\n",
      "refreshing...\n",
      "test set:Loss: 0.66050 | Acc: 58.31910% (3074/5271)\n",
      "\n",
      "training Epoch: 28\n",
      "refreshing...\n",
      "train set:Loss: 0.66067 | Acc: 59.16395% (15611/26386)\n",
      "testing Epoch: 28\n",
      "refreshing...\n",
      "test set:Loss: 0.66453 | Acc: 58.98312% (3109/5271)\n",
      "\n",
      "training Epoch: 29\n",
      "refreshing...\n",
      "train set:Loss: 0.65998 | Acc: 58.33396% (15392/26386)\n",
      "testing Epoch: 29\n",
      "test set:Loss: 0.66640 | Acc: 58.79836% (3161/5376)\n",
      "\n",
      "training Epoch: 30\n",
      "refreshing...\n",
      "train set:Loss: 0.65314 | Acc: 58.84939% (15528/26386)\n",
      "testing Epoch: 30\n",
      "refreshing...\n",
      "test set:Loss: 0.66252 | Acc: 57.92070% (3053/5271)\n",
      "\n",
      "training Epoch: 31\n",
      "refreshing...\n",
      "train set:Loss: 0.66032 | Acc: 58.57273% (15455/26386)\n",
      "testing Epoch: 31\n",
      "refreshing...\n",
      "test set:Loss: 0.65684 | Acc: 59.26769% (3124/5271)\n",
      "\n",
      "training Epoch: 32\n",
      "refreshing...\n",
      "train set:Loss: 0.65233 | Acc: 59.52778% (15707/26386)\n",
      "testing Epoch: 32\n",
      "test set:Loss: 0.65952 | Acc: 58.85417% (3164/5376)\n",
      "\n",
      "training Epoch: 33\n",
      "refreshing...\n",
      "refreshing...\n",
      "train set:Loss: 0.65657 | Acc: 58.87257% (15394/26148)\n",
      "testing Epoch: 33\n",
      "refreshing...\n",
      "test set:Loss: 0.68572 | Acc: 56.97211% (3003/5271)\n",
      "\n",
      "training Epoch: 34\n",
      "refreshing...\n",
      "train set:Loss: 0.65258 | Acc: 58.95551% (15556/26386)\n",
      "testing Epoch: 34\n",
      "refreshing...\n",
      "test set:Loss: 0.64903 | Acc: 59.47638% (3135/5271)\n",
      "\n",
      "training Epoch: 35\n",
      "refreshing...\n",
      "train set:Loss: 0.64948 | Acc: 59.28902% (15644/26386)\n",
      "testing Epoch: 35\n",
      "refreshing...\n",
      "test set:Loss: 0.66137 | Acc: 58.45191% (3081/5271)\n",
      "\n",
      "training Epoch: 36\n",
      "refreshing...\n",
      "train set:Loss: 0.64856 | Acc: 59.22459% (15627/26386)\n",
      "testing Epoch: 36\n",
      "test set:Loss: 0.65723 | Acc: 58.33333% (3136/5376)\n",
      "\n",
      "training Epoch: 37\n",
      "refreshing...\n",
      "train set:Loss: 0.64730 | Acc: 59.56947% (15718/26386)\n",
      "testing Epoch: 37\n",
      "refreshing...\n",
      "test set:Loss: 0.65711 | Acc: 57.10491% (3010/5271)\n",
      "\n",
      "training Epoch: 38\n",
      "refreshing...\n",
      "train set:Loss: 0.64618 | Acc: 59.59221% (15724/26386)\n",
      "testing Epoch: 38\n",
      "refreshing...\n",
      "test set:Loss: 0.65556 | Acc: 59.66610% (3145/5271)\n",
      "\n",
      "training Epoch: 39\n",
      "refreshing...\n",
      "train set:Loss: 0.64805 | Acc: 59.53915% (15710/26386)\n",
      "testing Epoch: 39\n",
      "test set:Loss: 0.66661 | Acc: 58.01711% (3119/5376)\n",
      "\n",
      "training Epoch: 40\n",
      "refreshing...\n",
      "train set:Loss: 0.64798 | Acc: 58.86834% (15533/26386)\n",
      "testing Epoch: 40\n",
      "refreshing...\n",
      "test set:Loss: 0.66194 | Acc: 58.30013% (3073/5271)\n",
      "\n",
      "training Epoch: 41\n",
      "refreshing...\n",
      "train set:Loss: 0.64939 | Acc: 58.96688% (15559/26386)\n",
      "testing Epoch: 41\n",
      "refreshing...\n",
      "test set:Loss: 0.65500 | Acc: 58.47088% (3082/5271)\n",
      "\n",
      "training Epoch: 42\n",
      "refreshing...\n",
      "train set:Loss: 0.64756 | Acc: 59.35345% (15661/26386)\n",
      "testing Epoch: 42\n",
      "refreshing...\n",
      "test set:Loss: 0.65413 | Acc: 59.24872% (3123/5271)\n",
      "\n",
      "training Epoch: 43\n",
      "refreshing...\n",
      "train set:Loss: 0.64851 | Acc: 59.24354% (15632/26386)\n",
      "testing Epoch: 43\n",
      "test set:Loss: 0.65860 | Acc: 58.72396% (3157/5376)\n",
      "\n",
      "training Epoch: 44\n",
      "refreshing...\n",
      "train set:Loss: 0.64775 | Acc: 59.83097% (15787/26386)\n",
      "testing Epoch: 44\n",
      "refreshing...\n",
      "test set:Loss: 0.66253 | Acc: 58.92620% (3106/5271)\n",
      "\n",
      "training Epoch: 45\n",
      "refreshing...\n",
      "train set:Loss: 0.64967 | Acc: 59.17911% (15615/26386)\n",
      "testing Epoch: 45\n",
      "refreshing...\n",
      "test set:Loss: 0.65269 | Acc: 59.09695% (3115/5271)\n",
      "\n",
      "training Epoch: 46\n",
      "refreshing...\n",
      "train set:Loss: 0.64534 | Acc: 59.56568% (15717/26386)\n",
      "testing Epoch: 46\n",
      "refreshing...\n",
      "test set:Loss: 0.66244 | Acc: 56.23221% (2964/5271)\n",
      "\n",
      "training Epoch: 47\n",
      "refreshing...\n",
      "train set:Loss: 0.64455 | Acc: 59.44819% (15686/26386)\n",
      "testing Epoch: 47\n",
      "test set:Loss: 0.65495 | Acc: 59.59821% (3204/5376)\n",
      "\n",
      "training Epoch: 48\n",
      "refreshing...\n",
      "train set:Loss: 0.64534 | Acc: 59.50125% (15700/26386)\n",
      "testing Epoch: 48\n",
      "refreshing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set:Loss: 0.65281 | Acc: 58.16733% (3066/5271)\n",
      "\n",
      "training Epoch: 49\n",
      "refreshing...\n",
      "train set:Loss: 0.64792 | Acc: 59.06162% (15584/26386)\n",
      "testing Epoch: 49\n",
      "refreshing...\n",
      "test set:Loss: 0.65427 | Acc: 57.99659% (3057/5271)\n",
      "\n",
      "training Epoch: 50\n",
      "refreshing...\n",
      "train set:Loss: 0.64577 | Acc: 59.55431% (15714/26386)\n",
      "testing Epoch: 50\n",
      "test set:Loss: 0.65388 | Acc: 58.66815% (3154/5376)\n",
      "\n",
      "training Epoch: 51\n",
      "refreshing...\n",
      "train set:Loss: 0.64798 | Acc: 59.06920% (15586/26386)\n",
      "testing Epoch: 51\n",
      "refreshing...\n",
      "test set:Loss: 0.65346 | Acc: 58.58471% (3088/5271)\n",
      "\n",
      "training Epoch: 52\n",
      "refreshing...\n",
      "train set:Loss: 0.64694 | Acc: 59.08057% (15589/26386)\n",
      "testing Epoch: 52\n",
      "refreshing...\n",
      "test set:Loss: 0.65352 | Acc: 59.55227% (3139/5271)\n",
      "\n",
      "training Epoch: 53\n",
      "refreshing...\n",
      "train set:Loss: 0.64605 | Acc: 59.63390% (15735/26386)\n",
      "testing Epoch: 53\n",
      "refreshing...\n",
      "test set:Loss: 0.65084 | Acc: 58.43293% (3080/5271)\n",
      "\n",
      "training Epoch: 54\n",
      "refreshing...\n",
      "train set:Loss: 0.64458 | Acc: 59.45577% (15688/26386)\n",
      "testing Epoch: 54\n",
      "test set:Loss: 0.65222 | Acc: 58.53795% (3147/5376)\n",
      "\n",
      "training Epoch: 55\n",
      "refreshing...\n",
      "train set:Loss: 0.66121 | Acc: 57.08330% (15062/26386)\n",
      "testing Epoch: 55\n",
      "refreshing...\n",
      "test set:Loss: 0.66558 | Acc: 58.98312% (3109/5271)\n",
      "\n",
      "training Epoch: 56\n",
      "refreshing...\n",
      "train set:Loss: 0.66194 | Acc: 58.78496% (15511/26386)\n",
      "testing Epoch: 56\n",
      "refreshing...\n",
      "test set:Loss: 0.66240 | Acc: 58.56574% (3087/5271)\n",
      "\n",
      "training Epoch: 57\n",
      "refreshing...\n",
      "train set:Loss: 0.65713 | Acc: 59.61116% (15729/26386)\n",
      "testing Epoch: 57\n",
      "refreshing...\n",
      "test set:Loss: 0.66353 | Acc: 58.39499% (3078/5271)\n",
      "\n",
      "training Epoch: 58\n",
      "refreshing...\n",
      "train set:Loss: 0.65790 | Acc: 59.37239% (15666/26386)\n",
      "testing Epoch: 58\n",
      "test set:Loss: 0.66278 | Acc: 57.86830% (3111/5376)\n",
      "\n",
      "training Epoch: 59\n",
      "refreshing...\n",
      "train set:Loss: 0.65462 | Acc: 59.64527% (15738/26386)\n",
      "testing Epoch: 59\n",
      "refreshing...\n",
      "test set:Loss: 0.65379 | Acc: 59.26769% (3124/5271)\n",
      "\n",
      "training Epoch: 60\n",
      "refreshing...\n",
      "train set:Loss: 0.64914 | Acc: 58.72432% (15495/26386)\n",
      "testing Epoch: 60\n",
      "refreshing...\n",
      "test set:Loss: 0.64933 | Acc: 59.74198% (3149/5271)\n",
      "\n",
      "training Epoch: 61\n",
      "refreshing...\n",
      "train set:Loss: 0.65074 | Acc: 58.76601% (15506/26386)\n",
      "testing Epoch: 61\n",
      "test set:Loss: 0.65970 | Acc: 58.98438% (3171/5376)\n",
      "\n",
      "training Epoch: 62\n",
      "refreshing...\n",
      "train set:Loss: 0.66273 | Acc: 58.33775% (15393/26386)\n",
      "testing Epoch: 62\n",
      "refreshing...\n",
      "test set:Loss: 0.66672 | Acc: 58.26219% (3071/5271)\n",
      "\n",
      "training Epoch: 63\n",
      "refreshing...\n",
      "train set:Loss: 0.66219 | Acc: 59.10710% (15596/26386)\n",
      "testing Epoch: 63\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-986c34ca69fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-75b5cc3f7c48>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m257\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-295fecf305bd>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#         print(idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mlandmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmarks_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m# landmarks = self.landmarks_frame.ix[idx, 1:].as_matrix().astype('float')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1104\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0mtipo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_dtype_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     return (issubclass(tipo, np.integer) and\n\u001b[0;32m--> 821\u001b[0;31m             not issubclass(tipo, (np.datetime64, np.timedelta64)))\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36599, 258)\n"
     ]
    }
   ],
   "source": [
    "landmarks_frame = pd.read_csv('./dataset.csv')\n",
    "# './trainset.csv'\n",
    "\n",
    "print(landmarks_frame.shape)\n",
    "# landmarks_frame[:10].to_csv('result1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
